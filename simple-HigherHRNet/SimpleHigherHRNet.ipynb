{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a4fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cpu'\n",
      "device: 'cpu'\n",
      "(3, 17, 3)\n",
      "[[[132.08008    239.99023      0.89205015]\n",
      "  [127.197266   245.36133      0.91190183]\n",
      "  [127.197266   235.10742      0.9095003 ]\n",
      "  [132.56836    254.15039      0.75639826]\n",
      "  [131.5918     228.75977      0.54365265]\n",
      "  [173.58398    260.49805      0.7684961 ]\n",
      "  [170.6543     218.99414      0.78835243]\n",
      "  [230.22461    261.4746       0.7744674 ]\n",
      "  [219.9707     213.13477      0.80852056]\n",
      "  [216.06445    252.68555      0.76252925]\n",
      "  [263.42773    208.74023      0.8000913 ]\n",
      "  [274.16992    249.26758      0.5883349 ]\n",
      "  [270.26367    220.94727      0.58154714]\n",
      "  [347.4121     240.9668       0.7183753 ]\n",
      "  [348.87695    228.27148      0.70819205]\n",
      "  [424.56055    236.08398      0.49525902]\n",
      "  [423.58398    227.29492      0.6276552 ]]\n",
      "\n",
      " [[130.12695    340.57617      0.8639339 ]\n",
      "  [124.75586    344.9707       0.87999827]\n",
      "  [125.24414    335.69336      0.87164426]\n",
      "  [128.66211    351.80664      0.86685205]\n",
      "  [129.63867    329.3457       0.86258364]\n",
      "  [168.21289    364.99023      0.74538493]\n",
      "  [170.16602    320.55664      0.77483666]\n",
      "  [214.59961    378.17383      0.6588135 ]\n",
      "  [216.06445    306.88477      0.74452925]\n",
      "  [220.45898    371.82617      0.7637856 ]\n",
      "  [264.89258    301.0254       0.77426124]\n",
      "  [274.16992    361.08398      0.49559844]\n",
      "  [271.72852    328.85742      0.56509286]\n",
      "  [347.9004     351.31836      0.6803931 ]\n",
      "  [344.9707     323.9746       0.64773875]\n",
      "  [419.67773    346.43555      0.6617425 ]\n",
      "  [410.88867    318.60352      0.7047751 ]]\n",
      "\n",
      " [[ 98.87695    121.82617      0.85581636]\n",
      "  [ 93.99414    126.2207       0.8789526 ]\n",
      "  [ 93.01758    117.43164      0.87354887]\n",
      "  [ 99.853516   132.56836      0.8474808 ]\n",
      "  [ 97.90039    110.10742      0.8901085 ]\n",
      "  [144.77539    145.26367      0.76207125]\n",
      "  [142.33398     93.50586      0.73855335]\n",
      "  [197.02148    157.95898      0.7819357 ]\n",
      "  [193.60352     80.81055      0.8197264 ]\n",
      "  [248.7793     164.79492      0.7539877 ]\n",
      "  [239.99023     70.55664      0.7567158 ]\n",
      "  [253.66211    133.05664      0.5096144 ]\n",
      "  [251.70898     98.38867      0.52531236]\n",
      "  [337.1582     124.75586      0.66926444]\n",
      "  [341.06445    106.20117      0.6918707 ]\n",
      "  [418.70117    118.896484     0.5601354 ]\n",
      "  [417.23633    117.43164      0.48528814]]]\n",
      "(19, 2)\n",
      "[[15 13]\n",
      " [13 11]\n",
      " [16 14]\n",
      " [14 12]\n",
      " [11 12]\n",
      " [ 5 11]\n",
      " [ 6 12]\n",
      " [ 5  6]\n",
      " [ 5  7]\n",
      " [ 6  8]\n",
      " [ 7  9]\n",
      " [ 8 10]\n",
      " [ 1  2]\n",
      " [ 0  1]\n",
      " [ 0  2]\n",
      " [ 1  3]\n",
      " [ 2  4]\n",
      " [ 0  5]\n",
      " [ 0  6]]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from models.higherhrnet import HigherHRNet\n",
    "from misc.HeatmapParser import HeatmapParser\n",
    "from misc.utils import get_multi_scale_size, resize_align_multi_scale, get_multi_stage_outputs, aggregate_results, get_final_preds, bbox_iou\n",
    "from misc.visualization import joints_dict\n",
    "from misc.visualization import draw_points_and_skeleton as dps\n",
    "from misc.visualization import draw_skeleton\n",
    "from datetime import datetime\n",
    "\n",
    "start=datetime.now()\n",
    "\n",
    "class SimpleHigherHRNet:\n",
    "    \"\"\"\n",
    "    SimpleHigherHRNet class.\n",
    "\n",
    "    The class provides a simple and customizable method to load the HigherHRNet network, load the official pre-trained\n",
    "    weights, and predict the human pose on single images or a batch of images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 c,\n",
    "                 nof_joints,\n",
    "                 checkpoint_path,\n",
    "                 model_name='HigherHRNet',\n",
    "                 resolution=512,\n",
    "                 interpolation=cv2.INTER_LINEAR,\n",
    "                 return_heatmaps=True,\n",
    "                 return_bounding_boxes=True,\n",
    "                 filter_redundant_poses=True,\n",
    "                 max_nof_people=30,\n",
    "                 max_batch_size=32,\n",
    "                 device=torch.device(\"cpu\")):\n",
    "        \"\"\"\n",
    "        Initializes a new SimpleHigherHRNet object.\n",
    "        HigherHRNet is initialized on the torch.device(\"device\") and\n",
    "        its pre-trained weights will be loaded from disk.\n",
    "\n",
    "        Args:\n",
    "            c (int): number of channels (when using HigherHRNet model).\n",
    "            nof_joints (int): number of joints.\n",
    "            checkpoint_path (str): path to an official higherhrnet checkpoint.\n",
    "            model_name (str): model name (just HigherHRNet at the moment).\n",
    "                Valid names for HigherHRNet are: `HigherHRNet`, `higherhrnet`\n",
    "                Default: \"HigherHRNet\"\n",
    "            resolution (int): higherhrnet input resolution - format: int == min(width, height).\n",
    "                Default: 512\n",
    "            interpolation (int): opencv interpolation algorithm.\n",
    "                Default: cv2.INTER_LINEAR\n",
    "            return_heatmaps (bool): if True, heatmaps will be returned along with poses by self.predict.\n",
    "                Default: False\n",
    "            return_bounding_boxes (bool): if True, bounding boxes will be returned along with poses by self.predict.\n",
    "                Default: False\n",
    "            filter_redundant_poses (bool): if True, redundant poses (poses being almost identical) are filtered out.\n",
    "                Default: True\n",
    "            max_nof_people (int): maximum number of detectable people.\n",
    "                Default: 30\n",
    "            max_batch_size (int): maximum batch size used in higherhrnet inference.\n",
    "                Useless without multiperson=True.\n",
    "                Default: 16\n",
    "            device (:class:`torch.device` or str): the higherhrnet (and yolo) inference will be run on this device.\n",
    "                Default: torch.device(\"cpu\")\n",
    "        \"\"\"\n",
    "\n",
    "        self.c = c\n",
    "        self.nof_joints = nof_joints\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.model_name = model_name\n",
    "        self.resolution = resolution\n",
    "        self.interpolation = interpolation\n",
    "        self.return_heatmaps = return_heatmaps\n",
    "        self.return_bounding_boxes = return_bounding_boxes\n",
    "        self.filter_redundant_poses = filter_redundant_poses\n",
    "        self.max_nof_people = max_nof_people\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # assert nof_joints in (14, 15, 17)\n",
    "        if self.nof_joints == 14:\n",
    "            self.joint_set = 'crowdpose'\n",
    "        elif self.nof_joints == 15:\n",
    "            self.joint_set = 'mpii'\n",
    "        elif self.nof_joints == 17:\n",
    "            self.joint_set = 'coco'\n",
    "        else:\n",
    "            raise ValueError('Wrong number of joints.')\n",
    "\n",
    "        if model_name in ('HigherHRNet', 'higherhrnet'):\n",
    "            self.model = HigherHRNet(c=c, nof_joints=nof_joints)\n",
    "        else:\n",
    "            raise ValueError('Wrong model name.')\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        if 'model' in checkpoint:\n",
    "            checkpoint = checkpoint['model']\n",
    "        # fix issue with official high-resolution weights\n",
    "        checkpoint = OrderedDict([(k[2:] if k[:2] == '1.' else k, v) for k, v in checkpoint.items()])\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "\n",
    "        if 'cuda' in str(self.device):\n",
    "            print(\"device: 'cuda' - \", end=\"\")\n",
    "\n",
    "            if 'cuda' == str(self.device):\n",
    "                # if device is set to 'cuda', all available GPUs will be used\n",
    "                print(\"%d GPU(s) will be used\" % torch.cuda.device_count())\n",
    "                device_ids = None\n",
    "            else:\n",
    "                # if device is set to 'cuda:IDS', only that/those device(s) will be used\n",
    "                print(\"GPU(s) '%s' will be used\" % str(self.device))\n",
    "                device_ids = [int(x) for x in str(self.device)[5:].split(',')]\n",
    "\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=device_ids)\n",
    "        elif 'cpu' == str(self.device):\n",
    "            print(\"device: 'cpu'\")\n",
    "        else:\n",
    "            raise ValueError('Wrong device name.')\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.output_parser = HeatmapParser(num_joints=self.nof_joints,\n",
    "                                           joint_set=self.joint_set,\n",
    "                                           max_num_people=self.max_nof_people,\n",
    "                                           ignore_too_much=True,\n",
    "                                           detection_threshold=0.3)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        Predicts the human pose on a single image or a stack of n images.\n",
    "\n",
    "        Args:\n",
    "            image (:class:`np.ndarray`):\n",
    "                the image(s) on which the human pose will be estimated.\n",
    "\n",
    "                image is expected to be in the opencv format.\n",
    "                image can be:\n",
    "                    - a single image with shape=(height, width, BGR color channel)\n",
    "                    - a stack of n images with shape=(n, height, width, BGR color channel)\n",
    "\n",
    "        Returns:\n",
    "            :class:`np.ndarray` or list:\n",
    "                a numpy array containing human joints for each (detected) person.\n",
    "\n",
    "                Format:\n",
    "                    if image is a single image:\n",
    "                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n",
    "                    if image is a stack of n images:\n",
    "                        list of n np.ndarrays with\n",
    "                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n",
    "\n",
    "                Each joint has 3 values: (y position, x position, joint confidence).\n",
    "\n",
    "                If self.return_heatmaps, the class returns a list with (heatmaps, human joints)\n",
    "                If self.return_bounding_boxes, the class returns a list with (bounding boxes, human joints)\n",
    "                If self.return_heatmaps and self.return_bounding_boxes, the class returns a list with\n",
    "                    (heatmaps, bounding boxes, human joints)\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            return self._predict_single(image)\n",
    "        elif len(image.shape) == 4:\n",
    "            return self._predict_batch(image)\n",
    "        else:\n",
    "            raise ValueError('Wrong image format.')\n",
    "\n",
    "    def _predict_single(self, image):\n",
    "        ret = self._predict_batch(image[None, ...])\n",
    "        if len(ret) > 1:  # heatmaps and/or bboxes and joints\n",
    "            ret = [r[0] for r in ret]\n",
    "        else:  # joints only\n",
    "            ret = ret[0]\n",
    "        return ret\n",
    "\n",
    "    def _predict_batch(self, image):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            heatmaps_list = None\n",
    "            tags_list = []\n",
    "\n",
    "            # scales and base (size, center, scale)\n",
    "            scales = (1,)  # ToDo add support to multiple scales\n",
    "\n",
    "            scales = sorted(scales, reverse=True)\n",
    "            base_size, base_center, base_scale = get_multi_scale_size(\n",
    "                image[0], self.resolution, 1, 1\n",
    "            )\n",
    "\n",
    "            # for each scale (at the moment, just one scale)\n",
    "            for idx, scale in enumerate(scales):\n",
    "                # rescale image, convert to tensor, move to device\n",
    "                images = list()\n",
    "                for img in image:\n",
    "                    image, size_resized, _, _ = resize_align_multi_scale(\n",
    "                        img, self.resolution, scale, min(scales), interpolation=self.interpolation\n",
    "                    )\n",
    "                    image = self.transform(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).unsqueeze(dim=0)\n",
    "                    image = image.to(self.device)\n",
    "                    images.append(image)\n",
    "                images = torch.cat(images)\n",
    "\n",
    "                # inference\n",
    "                # output: list of HigherHRNet outputs (heatmaps)\n",
    "                # avg_heatmaps: averaged heatmaps\n",
    "                # tags: per-pixel identity ids.\n",
    "                #       See Newell et al., Associative Embedding: End-to-End Learning for Joint Detection and\n",
    "                #           Grouping, NIPS 2017. https://arxiv.org/abs/1611.05424 or\n",
    "                #           http://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping\n",
    "                outputs, heatmaps, tags = get_multi_stage_outputs(\n",
    "                    self.model, images, with_flip=False, project2image=True, size_projected=size_resized,\n",
    "                    nof_joints=self.nof_joints, max_batch_size=self.max_batch_size\n",
    "                )\n",
    "\n",
    "                # aggregate the multiple heatmaps and tags\n",
    "                heatmaps_list, tags_list = aggregate_results(\n",
    "                    scale, heatmaps_list, tags_list, heatmaps, tags, with_flip=False, project2image=True\n",
    "                )\n",
    "\n",
    "            heatmaps = heatmaps_list.float() / len(scales)\n",
    "            tags = torch.cat(tags_list, dim=4)\n",
    "\n",
    "            # refine prediction\n",
    "            # grouped has the shape (people, joints, 4) -> 4: (x, y, confidence, tag)\n",
    "            # scores has the shape (people, ) and corresponds to the person confidence before refinement\n",
    "            grouped, scores = self.output_parser.parse(\n",
    "                heatmaps, tags, adjust=True, refine=True  # ToDo parametrize these two parameters\n",
    "            )\n",
    "\n",
    "            # get final predictions\n",
    "            final_results = get_final_preds(\n",
    "                grouped, base_center, base_scale, [heatmaps.shape[3], heatmaps.shape[2]]\n",
    "            )\n",
    "\n",
    "            if self.filter_redundant_poses:\n",
    "                # filter redundant poses - this step filters out poses whose joints have, on average, a difference\n",
    "                #   lower than 3 pixels\n",
    "                # this is useful when refine=True in self.output_parser.parse because that step joins together\n",
    "                #   skeleton parts belonging to the same people (but then it does not remove redundant skeletons)\n",
    "                final_pts = []\n",
    "                # for each image\n",
    "                for i in range(len(final_results)):\n",
    "                    final_pts.insert(i, list())\n",
    "                    # for each person\n",
    "                    for pts in final_results[i]:\n",
    "                        if len(final_pts[i]) > 0:\n",
    "                            diff = np.mean(np.abs(np.array(final_pts[i])[..., :2] - pts[..., :2]), axis=(1, 2))\n",
    "                            if np.any(diff < 3):  # average diff between this pose and another one is less than 3 pixels\n",
    "                                continue\n",
    "                        final_pts[i].append(pts)\n",
    "                final_results = final_pts\n",
    "\n",
    "            pts = []\n",
    "            boxes = []\n",
    "            for i in range(len(final_results)):\n",
    "                pts.insert(i, np.asarray(final_results[i]))\n",
    "                if len(pts[i]) > 0:\n",
    "                    pts[i][..., [0, 1]] = pts[i][..., [1, 0]]  # restoring (y, x) order as in SimpleHRNet\n",
    "                    pts[i] = pts[i][..., :3]\n",
    "\n",
    "                    if self.return_bounding_boxes:\n",
    "                        left_top = np.min(pts[i][..., 0:2], axis=1)\n",
    "                        right_bottom = np.max(pts[i][..., 0:2], axis=1)\n",
    "                        # [x1, y1, x2, y2]\n",
    "                        boxes.insert(i, np.stack(\n",
    "                            [left_top[:, 1], left_top[:, 0], right_bottom[:, 1], right_bottom[:, 0]], axis=-1\n",
    "                        ))\n",
    "                else:\n",
    "                    boxes.insert(i, [])\n",
    "\n",
    "        res = list()\n",
    "        if self.return_heatmaps:\n",
    "            res.append(heatmaps)\n",
    "        if self.return_bounding_boxes:\n",
    "            res.append(boxes)\n",
    "        res.append(pts)\n",
    "\n",
    "        if len(res) > 1:\n",
    "            return res\n",
    "        else:\n",
    "            return res[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hhrnet = SimpleHigherHRNet(\n",
    "        c=32, nof_joints=17, checkpoint_path='./weights/pose_higher_hrnet_w32_512.pth',\n",
    "        resolution=512, device='cpu'\n",
    "    )\n",
    "    # img = np.ones((384, 256, 3), dtype=np.uint8)\n",
    "  \n",
    "    import cv2\n",
    "    from SimpleHigherHRNet import SimpleHigherHRNet\n",
    "\n",
    "    \n",
    "    model = SimpleHigherHRNet(32, 17, \"./weights/pose_higher_hrnet_w32_512.pth\")\n",
    "    image = cv2.imread('./Data/sample5.jpg', cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (500,500), interpolation = cv2.INTER_AREA)\n",
    "    joints = model.predict(image)\n",
    "\n",
    "    fjoint=joints_dict()\n",
    "    test=fjoint['coco']['skeleton']\n",
    "    skeleton = np.array(test)\n",
    "\n",
    "    file = open(\"keypoint.txt\", \"w\")\n",
    "    file2 = open(\"summary.txt\", \"w\")\n",
    "    a,b,c=joints.shape\n",
    "    file2.write(\"Number of people in picture:\\n\")\n",
    "    file2.write(str(a))\n",
    "    \n",
    "    for i, pt in enumerate(joints):\n",
    "        nof_j,temp=pt.shape\n",
    "        sta1=\"[\"\n",
    "        sta2=i\n",
    "        sta3=\"]\"\n",
    "        st=sta1+str(sta2)+sta3\n",
    "        file.write(str(st))\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "\n",
    "        for j in range(nof_j):\n",
    "            L=joints[i,j]\n",
    "            file.write(str(L))\n",
    "        file.write(\"\\n\")\n",
    "           \n",
    "                \n",
    "    file.close()\n",
    "            \n",
    "    print(joints.shape)\n",
    "    print(joints)\n",
    "    print(skeleton.shape)\n",
    "    print(skeleton)\n",
    "\n",
    "    newimage=dps(image,joints,skeleton)\n",
    "    cv2.imshow('img',newimage)\n",
    "    cv2.imwrite('output/output.jpg',newimage)\n",
    "    file2.write(\"\\nTime:\\n\")\n",
    "    t=datetime.now()-start\n",
    "    file2.write(str(t))\n",
    "    file2.close()\n",
    "\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff56bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
